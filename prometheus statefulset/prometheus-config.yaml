apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  bucket.yaml: |
    type: S3
    config:
      bucket: "gcl-download"
      endpoint: "gcl-download.oss-cn-beijing.aliyuncs.com"
      access_key: "xxxxxxxxxxxxxx"
      insecure: false
      signature_version2: false
      secret_key: "xxxxxxxxxxxxxxxxxx"
      http_config:
        idle_conn_timeout: 0s

  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s

      external_labels:
         monitor: 'k8s-sh-prod'
         service: 'k8s-all'
         ID: 'ID_NUM'
         
    remote_write:
      - url: "http://vmstorage:8400/insert/0/prometheus/"
    remote_read:
      - url: "http://vmstorage:8401/select/0/prometheus"
         
    scrape_configs:
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https 
   
    - job_name: 'kubernetes-cadvisor'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        #ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      #bearer_token: monitoring
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__meta_kubernetes_node_address_InternalIP]
        regex: (.+)
        target_label: __address__
        replacement: ${1}:10250
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /metrics/cadvisor
      - source_labels: [__meta_kubernetes_node_name]
        modulus:       10
        target_label:  __tmp_hash
        action:        hashmod
      - source_labels: [__tmp_hash]
        regex:         ID_NUM
        action:        keep
      metric_relabel_configs:
      - source_labels: [container]
        regex: (.+)
        target_label: container_name
        replacement: $1
        action: replace
      - source_labels: [pod]
        regex: (.+)
        target_label: pod_name
        replacement: $1
        action: replace
    
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        #ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      #bearer_token: monitoring
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__meta_kubernetes_node_address_InternalIP]
        regex: (.+)
        target_label: __address__
        replacement: ${1}:10250
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /metrics
      - source_labels: [__meta_kubernetes_node_name]
        modulus:       10
        target_label:  __tmp_hash
        action:        hashmod
      - source_labels: [__tmp_hash]
        regex:         ID_NUM
        action:        keep
      metric_relabel_configs:
      - source_labels: [container]
        regex: (.+)
        target_label: container_name
        replacement: $1
        action: replace
      - source_labels: [pod]
        regex: (.+)
        target_label: pod_name
        replacement: $1
        action: replace

    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - monitoring
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - monitoring
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

    - job_name: 'ingress-nginx-endpoints'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - nginx-ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2

    job_name: 'node_exporter'
    consul_sd_configs:
    - server: 'consul_server:8500'
    relabel_configs:
        - source_labels: [__address__]
        modulus:       10
        target_label:  __tmp_hash
        action:        hashmod
        - source_labels: [__tmp_hash]
        regex:         ID_NUM
        action:        keep
        - source_labels: [__tmp_hash]
        regex:       '(.*)'
        replacement: '${1}'
        target_label: hash_num
        - source_labels: [__meta_consul_tags]
        regex: .*test.*
        action: drop
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){0}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){1}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){2}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){3}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){4}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){5}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){6}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'
        - source_labels: [__meta_consul_tags]
        regex: ',(?:[^,]+,){7}([^=]+)=([^,]+),.*'
        replacement: '${2}'
        target_label: '${1}'


  k8s_cluster_rule.yaml: |+
    groups:
    - name: pod_etcd_monitor
      rules:
      - alert: pod_etcd_num_is_changing
        expr: sum(kube_pod_info{pod=~"etcd.*"})by(monitor) < 3
        for: 1m
        labels:
          level: high
          service: etcd
        annotations:
          summary: "集群:{{ $labels.monitor }},etcd集群pod低于正常总数"
          description: "总数为3,当前值是{{ $value}}"
    - name: pod_scheduler_monitor
      rules:
      - alert: pod_scheduler_num_is_changing
        expr: sum(kube_pod_info{pod=~"kube-scheduler.*"})by(monitor) < 3
        for: 1m
        labels:
          level: high
          service: scheduler
        annotations:
          summary: "集群:{{ $labels.monitor }},scheduler集群pod低于正常总数"
          description: "总数为3,当前值是{{ $value}}"
    - name: pod_controller_monitor
      rules:
      - alert: pod_controller_num_is_changing
        expr: sum(kube_pod_info{pod=~"kube-controller-manager.*"})by(monitor) < 3
        for: 1m
        labels:
          level: high
          service: controller
        annotations:
          summary: "集群:{{ $labels.monitor }},controller集群pod低于正常总数"
          description: "总数为3,当前值是{{ $value}}"
    - name: pod_apiserver_monitor
      rules:
      - alert: pod_apiserver_num_is_changing
        expr: sum(kube_pod_info{pod=~"kube-apiserver.*"})by(monitor) < 3
        for: 1m
        labels:
          level: high
          service: controller
        annotations:
          summary: "集群:{{ $labels.monitor }},apiserver集群pod低于正常总数"
          description: "总数为3,当前值是{{ $value}}"

  k8s_master_resource_rules.yaml: |+
    groups:
    - name: node_cpu_resource_monitor
      rules:
      - alert: 节点CPU使用量
        expr:  sum(kube_pod_container_resource_requests_cpu_cores{node=~".*"})by(node)/sum(kube_node_status_capacity_cpu_cores{node=~".*"})by(node)>0.7
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群NODE节点总的CPU使用核数已经超过了70%"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!"
    - name: node_memory_resource_monitor
      rules:
      - alert: 节点内存使用量
        expr:  sum(kube_pod_container_resource_limits_memory_bytes{node=~".*"})by(node)/sum(kube_node_status_capacity_memory_bytes{node=~".*"})by(node)>0.7
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群NODE节点总的memory使用核数已经超过了70%"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!"
    - name: 节点POD使用率
      rules:
      - alert: 节点pod使用率
        expr: sum by(node,monitor) (kube_pod_info{node=~".*"}) / sum by(node,monitor) (kube_node_status_capacity_pods{node=~".*"})> 0.9
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群NODE节点总的POD使用数量已经超过了90%"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!"      
    - name: master_cpu_used
      rules:
      - alert: 主节点CPU使用率
        expr:  sum(kube_pod_container_resource_limits_cpu_cores{node=~'master.*'})by(node)/sum(kube_node_status_capacity_cpu_cores{node=~'master.*'})by(node)>0.7
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群Master节点总的CPU申请核数已经超过了0.7,当前值为{{ $value }}!"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!" 
    - name: master_memory_resource_monitor
      rules:
      - alert: 主节点内存使用率
        expr:  sum(kube_pod_container_resource_limits_memory_bytes{node=~'master.*'})by(node)/sum(kube_node_status_capacity_memory_bytes{node=~'master.*'})by(node)>0.7
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群Master节点总的内存使用量已经超过了70%"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!"
    - name: master_pod_resource_monitor
      rules:
      - alert: 主节点POD使用率
        expr: sum(kube_pod_info{node=~"master.*"}) by (node) / sum(kube_node_status_capacity_pods{node=~"master.*"}) by (node)>0.7
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群Master节点总的POD使用数量已经超过了70%"
          description: "集群:{{ $labels.monitor }},节点:{{ $labels.node }}当前值为{{ $value }}!"     
  k8s_node_rule.yaml: |+
    groups:
    - name: K8sNodeMonitor
      rules:
      - alert: 集群节点资源监控
        expr: kube_node_status_condition{condition=~"OutOfDisk|MemoryPressure|DiskPressure",status!="false"} ==1
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群节点内存或磁盘资源短缺"
          description: "节点:{{ $labels.node }},集群:{{ $labels.monitor }},原因:{{ $labels.condition }}"
      - alert: 集群节点状态监控
        expr: sum(kube_node_status_condition{condition="Ready",status!="true"})by(node)  == 1
        for: 2m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "集群节点状态出现错误"
          description: "节点:{{ $labels.node }},集群:{{ $labels.monitor }}"
      - alert: 集群POD状态监控
        expr: sum (kube_pod_container_status_terminated_reason{reason!~"Completed|Error"})  by (pod,reason) ==1
        for: 1m
        labels:
          level: high
          service: pod
        annotations:
          summary: "集群pod状态出现错误"
          description: "集群:{{ $labels.monitor }},名称:{{ $labels.pod }},原因:{{ $labels.reason}}"
      - alert: 集群节点CPU使用监控
        expr:  sum(node_load1) BY (instance) / sum(rate(node_cpu_seconds_total[1m])) BY (instance) > 2
        for: 5m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "机器出现cpu平均负载过高"
          description: "节点: {{ $labels.instance }}平均每核大于2"
      - alert: NodeMemoryOver80Percent
        expr:  (1 - avg by (instance)(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))* 100 >85
        for: 1m
        labels:
          level: disaster
          service: node
        annotations:
          summary: "机器出现内存使用超过85%"
          description: "节点: {{ $labels.instance }}"
  k8s_pod_rule.yaml: |+
    groups:
      - name: pod_status_monitor
        rules:
        - alert: pod错误状态监控
          expr: changes(kube_pod_status_phase{phase=~"Failed"}[10m]) >0
          for: 5m
          labels:
            level: high
            service: pod-failed
          annotations:
            summary: "集群:{{ $labels.monitor }}存在pod状态异常"
            description: "pod:{{$labels.pod}},状态:{{$labels.phase}}"
        - alert: pod异常状态监控
          expr: sum(kube_pod_status_phase{phase="Pending"})by(namespace,pod,phase)>0
          for: 1m
          labels:
            level: high
            service: pod-pending
          annotations:
            summary: "集群:{{ $labels.monitor }}存在pod状态pening异常超10分钟"
            description: "pod:{{$labels.pod}},状态:{{$labels.phase}}"
        - alert: pod等待状态监控
          expr: sum(kube_pod_container_status_waiting_reason{reason!="ContainerCreating"})by(namespace,pod,reason)>0
          for: 1m
          labels:
            level: high
            service: pod-wait
          annotations:
            summary: "集群:{{ $labels.monitor }}存在pod状态Wait异常超5分钟"
            description: "pod:{{$labels.pod}},状态:{{$labels.reason}}"
        - alert: pod非正常状态监控
          expr: sum(kube_pod_container_status_terminated_reason{reason!="Completed"})by(namespace,pod,reason)>0
          for: 5m
          labels:
            level: high
            service: pod-nocom
          annotations:
            summary: "集群:{{ $labels.monitor }}存在pod状态Terminated异常超5分钟"
            description: "pod:{{$labels.pod}},状态:{{$labels.reason}}"
        - alert: pod重启监控
          expr: changes(kube_pod_container_status_restarts_total[20m])>3
          for: 5m
          labels:
            level: high
            service: pod-restart
          annotations:
            summary: "集群:{{ $labels.monitor }}存在pod半小时之内重启次数超过3次!"
            description: "pod:{{$labels.pod}}"
      - name: deployment_replicas_monitor
        rules:
        - alert: deployment监控
          expr: sum(kube_deployment_status_replicas_unavailable)by(namespace,deployment) >2
          for: 1m
          labels:
            level: high
            service: deployment-replicas
          annotations:
            summary: "集群:{{ $labels.monitor }},deployment:{{$labels.deployment}} 副本数未达到期望值! "
            description: "空间:{{$labels.namespace}}，当前不可用副本:{{$value}},请检查"
      - name: daemonset_replicas_monitor
        rules:
        - alert: Daemonset监控
          expr: sum(kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled)by(daemonset,namespace) >2
          for: 1m
          labels:
            level: high
            service: daemonset
          annotations:
            summary: "集群:{{ $labels.monitor }},daemonset:{{$labels.daemonset}} 守护进程数未达到期望值!"
            description: "空间:{{$labels.namespace}},当前不可用副本:{{$value}},请检查"
      - name: satefulset_replicas_monitor
        rules:
        - alert: Satefulset监控
          expr: kube_statefulset_replicas - kube_statefulset_status_replicas_ready >2
          for: 1m
          labels:
            level: high
            service: statefulset
          annotations:
            summary: "集群:{{ $labels.monitor }},statefulset:{{$labels.statefulset}} 副本数未达到期望值!"
            description: "空间:{{$labels.namespace}}，当前不可用副本:{{$value}},请检查"
      - name: pvc_replicas_monitor
        rules:
        - alert: PVC监控
          expr: kube_persistentvolumeclaim_status_phase{phase!="Bound"} == 1
          for: 5m
          labels:
            level: high
            service: pvc
          annotations:
            summary: "集群:{{ $labels.monitor }},statefulset:{{$labels.persistentvolumeclaim}} 异常未bound成功!"
            description: "pvc出现异常"
      - name: K8sClusterJob
        rules:	  
        - alert: 集群JOB状态监控
          expr: sum(kube_job_status_failed{job="kubernetes-service-endpoints",k8s_app="kube-state-metrics"})by(job_name) ==1
          for: 1m
          labels:
            level: disaster
            service: job
          annotations:
            summary: "集群存在执行失败的Job"
            description: "集群:{{ $labels.monitor }},名称:{{ $labels.job_name }}"
      - name: pod_container_cpu_resource_monitor
        rules:
        - alert: 容器内cpu占用监控
          expr: namespace:container_cpu_usage_seconds_total:sum_rate / sum(kube_pod_container_resource_limits_cpu_cores) by (monitor,namespace,pod_name)> 0.8
          for: 1m
          labels:
            level: high
            service: container_cpu
          annotations:
            summary: "集群:{{ $labels.monitor }} 出现Pod CPU使用率已经超过申请量的80%,"
            description: "namespace:{{$labels.namespace}}的pod:{{$labels.pod}},当前值为{{ $value }}"
        - alert: 容器内mem占用监控
          expr: namespace:container_memory_usage_bytes:sum/ sum(kube_pod_container_resource_limits_memory_bytes)by(monitor,namespace,pod_name) > 0.8
          for: 2m
          labels:
            level: high
            service: container_mem
          annotations:
            summary: "集群:{{ $labels.monitor }} 出现Pod memory使用率已经超过申请量的90%"
            description: "namespace:{{$labels.namespace}}的pod:{{$labels.pod}},当前值为{{ $value }}"
        
  redis_rules.yaml: |+
    groups:
    - name: k8s_container_rule
    rules:
    - expr: sum(rate(container_cpu_usage_seconds_total[5m])) by (monitor,namespace,pod_name)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
    - expr: sum(container_memory_usage_bytes{container_name="POD"}) by (monitor,namespace,pod_name)
        record: namespace:container_memory_usage_bytes:sum



